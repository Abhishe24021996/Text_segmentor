{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the tatoeba file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lns=[]\n",
    "# with open(\"sentences.csv\",'r',encoding='utf-8') as f:\n",
    "#     while True:\n",
    "#         line = f.readline()\n",
    "#         if not line:\n",
    "#             break\n",
    "#         if not re.search('\\seng\\s',line,re.I):\n",
    "#             continue\n",
    "#         try:\n",
    "#             line = line.split('\\t')\n",
    "#             line = line[2]\n",
    "#             lns.append(line)\n",
    "#         except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(lns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns = sorted(lns, key=len, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('english_sentences_tatoeba.txt','w',encoding='utf-8') as f:\n",
    "    for line in lns:\n",
    "        f.write('%s'%line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275343 ['Hi\\n', 'So?\\n', 'Ow!\\n', 'Ok.\\n', 'Oi!\\n', 'OK.\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('english_sentences_tatoeba.txt','r',encoding='utf-8') as f:\n",
    "    lns = f.readlines()\n",
    "print(len(lns),lns[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns = [l.replace('\\n','') for l in lns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using nltk to tokenize sentences furthur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns_n=[]\n",
    "for l in lns:\n",
    "    l = sent_tokenize(l)\n",
    "    lns_n.extend(l)\n",
    "lns_n = sorted(lns_n , key=len , reverse=False)\n",
    "print(len(lns))\n",
    "print(\"new_length::\",len(lns_n))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating boundaries \n",
    "a = (print(l) for l in lns_n if ' and ' in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(next(a))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns_pos = [pos_tag(l.split(' ')) for l in lns_n[10000:11800]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns_pos=[]\n",
    "for l in lns_n[10000:12000]:\n",
    "    l = word_tokenize(l)\n",
    "    if len(l)<4:\n",
    "        continue\n",
    "    l = pos_tag(l)\n",
    "    lns_pos.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lns_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lns_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(lines, max_sents_per_example=6, n_examples=1000):\n",
    "    \"\"\"\n",
    "        Generates training data for deepsegment from list of sentences.\n",
    "        Parameters:\n",
    "        lines (list): Base sentences for data generation.\n",
    "        max_sents_per_example (int): Maximum number of sentences to be combined to form a single paragraph.\n",
    "        \n",
    "        n_examples (int): Number of training examples to be generated.\n",
    "        \n",
    "        Returns:\n",
    "        list, list: Training data and corresponding labels in BIOU format.\n",
    "    \"\"\"\n",
    "    x, y = [], []\n",
    "    \n",
    "    for current_i in tqdm(range(n_examples)):\n",
    "        x.append([])\n",
    "        y.append([])\n",
    "\n",
    "        chosen_lines = []\n",
    "        for _ in range(random.randint(1, max_sents_per_example)):\n",
    "            chosen_lines.append(random.choice(lines))\n",
    "        \n",
    "        chosen_lines = [bad_sentence_generator(line, remove_punctuation=random.randint(0, 3)) for line in chosen_lines]\n",
    "        \n",
    "        for line in chosen_lines:\n",
    "            words = line.strip().split()\n",
    "            for word_i, word in enumerate(words):\n",
    "                x[-1].append(word)\n",
    "                label = 'O'\n",
    "                if word_i == 0:\n",
    "                    label = 'B-sent'\n",
    "                y[-1].append(label)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "def bad_sentence_generator(sent, remove_punctuation = None):\n",
    "    \"\"\"\n",
    "        Returns sentence with completely/ partially removed punctuation.\n",
    "        Parameters:\n",
    "        sent (str): Sentence on which the punctuation removal operation is performed.\n",
    "        \n",
    "        remove_punctuation (int): removing punctuation completely if remove_punctuation ==0 or ==1, removing punctuation till a randomly selected point if remove_punctuation ==2\n",
    "        Returns:\n",
    "        str: Sentence with modified punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    if not remove_punctuation:\n",
    "        remove_punctuation = random.randint(0, 3)\n",
    "\n",
    "    break_point = random.randint(1, len(sent)-2)\n",
    "    lower_case = random.randint(0, 2)\n",
    "\n",
    "    if remove_punctuation <= 1:\n",
    "        # removing punctuation completely if remove_punctuation ==0 or ==1\n",
    "        sent = re.sub('['+string.punctuation+']', '', sent)\n",
    "    \n",
    "    elif remove_punctuation == 2:\n",
    "        # removing punctuation till a randomly selected point if remove_punctuation ==2\n",
    "        if random.randint(0,1) == 0:\n",
    "            sent = re.sub('['+string.punctuation+']', '', sent[:break_point]) + sent[break_point:]\n",
    "        # removing punctuation after a randomly selected point if remove_punctuation ==2        \n",
    "        else:\n",
    "            sent = sent[:break_point] + re.sub('['+string.punctuation+']', '', sent[break_point:])    \n",
    "    \n",
    "    if lower_case <= 1:\n",
    "        # lower casing sentence \n",
    "        sent = sent.lower()\n",
    "    \n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi\\n',\n",
       " 'So?\\n',\n",
       " 'Ow!\\n',\n",
       " 'Ok.\\n',\n",
       " 'Oi!\\n',\n",
       " 'OK.\\n',\n",
       " 'Nu?\\n',\n",
       " 'No?\\n',\n",
       " 'No.\\n',\n",
       " 'No!\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import string\n",
    "string.punctuation\n",
    "lns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 16620.92it/s]\n",
      "100%|███████████████████████████████████| 1000/1000 [00:00<00:00, 25657.32it/s]\n"
     ]
    }
   ],
   "source": [
    "x, y = generate_data(lines=lns[10000:], max_sents_per_example=6, n_examples=10000)\n",
    "x_, y_ = generate_data(lines=lns[:10000], max_sents_per_example=6, n_examples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[:1],y[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove.6B.100d.txt',\n",
       " 'glove.6B.200d.txt',\n",
       " 'glove.6B.300d.txt',\n",
       " 'glove.6B.50d.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(filename):\n",
    "    \"\"\"Loads vocab from a file\n",
    "    Args:\n",
    "        filename: (string) the format of the file must be one word per line.\n",
    "    Returns:\n",
    "        d: dict[word] = index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        d = dict()\n",
    "        with open(filename) as f:\n",
    "            for idx, word in enumerate(f):\n",
    "                word = word.strip()\n",
    "                d[word] = idx\n",
    "\n",
    "    except IOError:\n",
    "        raise MyIOError(filename)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove embedding\n",
    "glove_path = \"glove/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word Vectors....\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def get_glove_vocab(path):\n",
    "    print('Loading word Vectors....')\n",
    "    word2vec = {}\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.array(values[1:],dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "    print('Found %s word vectors.' %len(word2vec))\n",
    "    return word2vec\n",
    "\n",
    "word2vec = get_glove_vocab(glove_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2776: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1ddfa37a2356>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedding_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove/glove.6B.100d.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-d76e74f8c6bf>\u001b[0m in \u001b[0;36mload_vocab\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                 \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2776: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "embedding_dict = load_vocab(\"glove/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(x, y, vx, vy, epochs, batch_size, save_folder, glove_path):\n",
    "#     \"\"\"\n",
    "#         Trains a deepsegment model.\n",
    "#         Parameters:\n",
    "#         x (list): x generated from generate_data\n",
    "#         y (list): y generated from generate_data\n",
    "#         vx (list): x generated from generate_data\n",
    "#         vy (list): y generated from generate_data\n",
    "#         epochs (int): Max number of epochs.\n",
    "        \n",
    "#         batch_size (int): batch_size\n",
    "#         save_folder (str): path for the directory where checkpoints should be saved.\n",
    "#         glove_path (str): path to 100d word vectors.\n",
    "        \n",
    "#     \"\"\"\n",
    "\n",
    "#     embeddings = load_vocab(glove_path)\n",
    "    \n",
    "#     checkpoint_path = os.path.join(save_folder, 'checkpoint')\n",
    "#     final_weights_path = os.path.join(save_folder, 'final_weights')\n",
    "#     params_path = os.path.join(save_folder, 'params')\n",
    "#     utils_path = os.path.join(save_folder, 'utils')    \n",
    "\n",
    "#     checkpoint = ModelCheckpoint(checkpoint_path, verbose=1, save_best_only=True, mode='max', monitor='f1')\n",
    "#     earlystop = EarlyStopping(patience=3, monitor='f1', mode='max')\n",
    "\n",
    "#     model = seqtag_keras.Sequence(embeddings=embeddings)\n",
    "    \n",
    "#     model.fit(x, y, x_valid=vx, y_valid=vy, epochs=epochs, batch_size=batch_size, callbacks=[checkpoint, earlystop])\n",
    "\n",
    "#     model.save(final_weights_path, params_path, utils_path)\n",
    "\n",
    "\n",
    "# lang_code_mapping = {\n",
    "#     'english': 'en',\n",
    "#     'french': 'fr',\n",
    "#     'italian': 'it'\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def intialize_placeholders():\n",
    "word_ids = tf.placeholder(tf.int32, shape=[None,None], name=\"word_id_placeholder\")\n",
    "sequence_length = tf.placeholder(tf.int32, shape=[None,], name=\"sequence_length_placholder\")\n",
    "label = tf.placeholder(tf.int32,shape=[None,None], name=\"labels\")\n",
    "#         dropout = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "#                         name=\"dropout\")\n",
    "lr = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                name=\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwords=20000,\n",
    "embedding_size = 100\n",
    "hidden_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-5746c6285532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         word_embeddings_ = tf.get_variable(name=\"word_embeddings_\",\n\u001b[0;32m      5\u001b[0m                                           \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                                           shape=[nwords,embedding_size])\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#     else:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#         word_embeddings_ = tf.variable(embedding_dict,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1500\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1243\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    774\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    774\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 718\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    191\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot convert %s to Dimension\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n\u001b[0;32m    195\u001b[0m           self._value != value):\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "def word_embedding(embedding_dict=None,nwords=10000,embedding_size=100):\n",
    "    with tf.variable_scope(\"word_embedding\"):\n",
    "        if embedding_dict is None:\n",
    "            word_embeddings_ = tf.get_variable(name=\"word_embeddings_\",\n",
    "                                              dtype=tf.float32,\n",
    "                                              shape=[nwords,embedding_size])\n",
    "        else:\n",
    "            word_embeddings_ = tf.variable(embedding_dict,\n",
    "                                           name=\"word_embeddings_\",\n",
    "                                           dtype=tf.float32,\n",
    "                                           shape=[nwords,embedding_size],\n",
    "                                           trainable=True\n",
    "                                           )\n",
    "            \n",
    "        word_embedding = tf.nn.embedding_lookup(word_embeddings_,\"words_ids\",name=\"word_embeddings\")\n",
    "\n",
    "    return word_embedding\n",
    "        \n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def logits_op(hidden_dim):\n",
    "with tf.variable_scope(\"bidirectional_lstm\"):\n",
    "    for_cell = tf.contrib.rnn.LSTMCell(hidden_dim)\n",
    "    bac_cell = tf.contrib.rnn.LSTMCell(hidden_dim)\n",
    "    (for_out, bac_out), _ = tf.nn.bidirectional_dynamic_rnn(for_cell, bac_cell,\n",
    "                                                            word_embeddings,\n",
    "                                                            sequence_length=sequence_length,\n",
    "                                                            dtype=tf.float32)\n",
    "    #shape >> [bs, sequence_length, 2EMD]\n",
    "    output = tf.concat([for_out, bac_out], axis=-1)\n",
    "\n",
    "#         output = tf.nn.dropout(output,)\n",
    "\n",
    "with tf.variable_scope(\"w_b\"):\n",
    "    W = tf.get_variable(name=\"W\",dtype=tf.float32,\n",
    "                        shape = [2*hidden_dim,ntags])\n",
    "    b = tf.get_variable(name=\"b\", dtype=tf.float32,\n",
    "                        shape = [ntags],\n",
    "                        initializer = tf.zeros_initializer())\n",
    "\n",
    "\n",
    "    nsteps = tf.shape(output)[1]\n",
    "    output = tf.reshape(output, [-1,2*hidden_dim])\n",
    "\n",
    "    pred = tf.matmul(output,W) + b\n",
    "    logits = tf.reshape(pred,[-1,nsteps,ntags])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pred_op():\n",
    "label_pred = tf.cast(tf.argmax(logits, axis=-1),tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_op():\n",
    "losses = tf.nn.sparse_softmax_crossentropy_with_logits(logits=logits,labels=labels)\n",
    "mask = tf.sequence_mask(self.sequence_lengths)\n",
    "losses = tf.boolean_mask(losses, mask)\n",
    "loss = tf.reduce_mean(losses)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_session():\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variable_initializers()\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    return sess, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_session():\n",
    "    if not os.path.exist('model_folder'):\n",
    "        os.makedirs('model_folder')\n",
    "    saver.save(sess, 'model_folder')\n",
    "\n",
    "def close_sess():\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(data,batch_size):\n",
    "    x_batch, y_batch = [], []\n",
    "    for (x,y) in data:\n",
    "        if len(x_batch) == batch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "        if type(x[0]) == tuple:\n",
    "            x = zip(*x)\n",
    "        x_batch += [x]\n",
    "        y_batch += [y]\n",
    "        \n",
    "    if len(x_batch) != 0:\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "    \"\"\"\n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type\n",
    "\n",
    "\n",
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "    \"\"\"\n",
    "    if NONE in tags:\n",
    "        default = tags[NONE]\n",
    "    else:\n",
    "        default = None\n",
    "        \n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(words):\n",
    "    fd, sequence_length = get_feed_dict(words) #dropout=1.0)\n",
    "    labels_pred = sess.run(label_pred, feed_dict=fd)\n",
    "    return labels_pred, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_tok, nlevels=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "        nlevels: \"depth\" of padding, for the case where we have characters ids\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    if nlevels == 1:\n",
    "        max_length = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, sequence_length = _pad_sequences(sequences,\n",
    "                                            pad_tok, max_length)\n",
    "\n",
    "    elif nlevels == 2:\n",
    "        max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "                               for seq in sequences])\n",
    "        sequence_padded, sequence_length = [], []\n",
    "        for seq in sequences:\n",
    "            # all words are same length now\n",
    "            sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\n",
    "            sequence_padded += [sp]\n",
    "            sequence_length += [sl]\n",
    "\n",
    "        max_length_sentence = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, _ = _pad_sequences(sequence_padded,\n",
    "                [pad_tok]*max_length_word, max_length_sentence)\n",
    "        sequence_length, _ = _pad_sequences(sequence_length, 0,\n",
    "                max_length_sentence)\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(words, labels=None, lr=None, dropout=None):\n",
    "        \"\"\"Given some data, pad it and build a feed dictionary\n",
    "        Args:\n",
    "            words: list of sentences. A sentence is a list of ids of a list of\n",
    "                words. A word is a list of ids\n",
    "            labels: list of ids\n",
    "            lr: (float) learning rate\n",
    "            dropout: (float) keep prob\n",
    "        Returns:\n",
    "            dict {placeholder: value}\n",
    "        \"\"\"\n",
    "        # perform padding of the given data\n",
    "#         if self.config.use_chars:\n",
    "#             char_ids, word_ids = zip(*words)\n",
    "#             word_ids, sequence_lengths = pad_sequences(word_ids, 0)\n",
    "#             char_ids, word_lengths = pad_sequences(char_ids, pad_tok=0,\n",
    "#                 nlevels=2)\n",
    "#         else:\n",
    "        word_ids, sequence_lengths = pad_sequences(words, 0)\n",
    "\n",
    "        # build feed dictionary\n",
    "        feed = {\n",
    "            \"word_id_placeholder\": word_ids,\n",
    "            \"sequence_length_placholder\": sequence_lengths\n",
    "        }\n",
    "\n",
    "#         if self.config.use_chars:\n",
    "#             feed[self.char_ids] = char_ids\n",
    "#             feed[self.word_lengths] = word_lengths\n",
    "        if labels is not None:\n",
    "            labels, _ = pad_sequences(labels, 0)\n",
    "            feed[\"labels\"] = labels\n",
    "\n",
    "        if lr is not None:\n",
    "            feed[\"lr\"] = lr\n",
    "\n",
    "#         if dropout is not None:\n",
    "#             feed[\"dropout\"] = dropout\n",
    "\n",
    "        return feed, sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(train, dev, epoch):\n",
    "    for i ,(words, labels) in enumerate(minibatches(train,batch_size)):\n",
    "        fd, _ = get_feed_dict(words, labels, lr)#, dropout)\n",
    "        _, train_los = sess.run(training_op, loss, feed_dict = fd)\n",
    "        print(\"training_loss::::\",train_los)\n",
    "        \n",
    "        metrics = evaluation(dev)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        print(msg)\n",
    "        print(metrics['f1'])\n",
    "        \n",
    "\n",
    "def evaluation(dev):\n",
    "    accs = []\n",
    "    correct_prediction, total_correct, total_predictions = 0., 0., 0.\n",
    "    for words, labels in minibatches(dev, batch_size):\n",
    "        label_preds, sequence_length = predict_batch(words)\n",
    "        \n",
    "        for lab, lab_pred, length in zip(labels, label_preds, sequence_length):\n",
    "            lab = lab[:length]\n",
    "            lab_pred = lab_pred[:length]\n",
    "            \n",
    "            accs += [a==b for (a,b) in zip(lab,lab_pred)]\n",
    "            \n",
    "            lab_chunks = set(get_chunks(lab, vocab_tags))\n",
    "            lab_pred_chunks = set(get_chunks(lab_pred, vocab_tags))\n",
    "            \n",
    "            correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "            total_preds   += len(lab_pred_chunks)\n",
    "            total_correct += len(lab_chunks)\n",
    "            \n",
    "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "    acc = np.mean(accs)\n",
    "\n",
    "    return {\"acc\": 100*acc, \"f1\": 100*f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train, dev, epochs, epoch_no_improvement=4):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch {epoch} out of {epochs}\")\n",
    "        score = run_epoch(train, dev, epoch)\n",
    "        lr *= 0.001#lr_decay # decay learning rate\n",
    "        if score >= best_score:\n",
    "            epo_no_improvement = 0\n",
    "            save_session()\n",
    "            best_score = score\n",
    "            print(\"Best Score\", best_score)\n",
    "        else:\n",
    "            epo_no_improvement += 1\n",
    "            if epo_no_improvement > epoch_no_improvement:\n",
    "                print(f\"Early Stoping {epo_no_improvement} with no Improvement\")\n",
    "                break\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build():\n",
    "    \"\"\"build model\"\"\"\n",
    "    tf.reset_default_graph()\n",
    "#     intialize_placeholders()\n",
    "#     word_embedding()\n",
    "#     logits_op()\n",
    "#     pred_op()\n",
    "#     loss_op()\n",
    "    initialize_session()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [(i,j) for i, j in zip(x,y)]\n",
    "dev_data = [(i,j) for i, j in zip(x_,y_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build()\n",
    "train(train=train_data, dev=dev_data, epochs = 25, epoch_no_improvement=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in a:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
