{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import nltk\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filename = \"data_folder/10_02_2020_OnlySyntax.txt\"\n",
    "#read eng sentences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tag_data(filename):\n",
    "    eng_lines = []\n",
    "    with open(filename,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().replace('<s>','').replace('</t>','').split('</s> <t>')\n",
    "            line = line[0].strip()\n",
    "            line = re.sub('\\s+',' ',line)\n",
    "            eng_lines.append(line)\n",
    "    return eng_lines\n",
    "\n",
    "lns = prepare_tag_data(filename=new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43805,\n",
       " ['poll MLSDFIELD bits of register MLSDREG until cleared',\n",
       "  'poll MLSDFIELD bits of register MLSDREG until cleared'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lns),lns[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build params (Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "DECAY_LR = 0.9\n",
    "OPTM = \"Adam\"\n",
    "DROPOUT = 0.3\n",
    "EPOCHS_NO_IMPROVEMENT = 4\n",
    "HIDDEN_DIM = 256\n",
    "CHAR_HIDDEN_DIM = 50\n",
    "DATA_FILEPATH = \"data_folder/data.txt\"\n",
    "GLOVE_FILEPATH = \"glove/glove.6B.100d.txt\"\n",
    "VOCAB_FILENAME = \"data_folder/vocabs.txt\"\n",
    "TAG_FILENAME = \"data_folder/tags.txt\"\n",
    "EMBEDDING_VOCAB = \"data_folder/vocab_embeddings.npz\"\n",
    "EMBEDDING_TAG = \"data_folder/tag_embeddings.npz\"\n",
    "CHAR_FILENAME = \"data_folder.char.txt\"\n",
    "\n",
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "NONE = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build files from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_batches(data,batch_size=32):\n",
    "    \"\"\"Args >> data=tuple(sentence,tag)\n",
    "    returns data in batches\"\"\"\n",
    "    steps = len(data)//batch_size\n",
    "    x_batch, y_batch = [], []\n",
    "    for (x,y) in data:\n",
    "        if len(x_batch) == batch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "        if type(x[0])==tuple:\n",
    "            x = zip(*x)\n",
    "            x_batch += [x]\n",
    "            y_batch += [y]\n",
    "    if len(x_batch) != 0:\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabs(datasets):\n",
    "    \"\"\"Building vocabs present in datasets\"\"\"\n",
    "    print(\"Building Vocab ....\")\n",
    "    vocab_words = set()\n",
    "    vocab_tags = set()\n",
    "    for dataset in datasets:\n",
    "        for words, tags in dataset:\n",
    "            vocab_words.update(words)\n",
    "            vocab_tags.update(tags)\n",
    "    print(\"-done vocab words {s}--- vocab_tags {d}\".format(s=len(vocab_words),d=len(vocab_tags)))\n",
    "    return vocab_words, vocab_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_vocab(dataset):\n",
    "    vocab_char = set()\n",
    "    for words, _ in dataset:\n",
    "        for word in words:\n",
    "            vocab_char.update(word)\n",
    "    return vocab_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_vocab(path):\n",
    "    \"\"\"Building vocabs present in glove\"\"\"\n",
    "    print(\"Building Vocab..glove...\")\n",
    "    vocab = set()\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line.strip().split(' ')[0]\n",
    "            vocab.add(word)\n",
    "    print(\"done. {} tokens in glove\".format(len(vocab)))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_vocab(vocab_words,vocab_glove,*args):\n",
    "    \"\"\"Combine word vocabs >> glove_vocabs+dataset_vocabs\"\"\"\n",
    "    vocab = vocab_words & vocab_glove\n",
    "    for word in args:\n",
    "        vocab.add(word)\n",
    "    return vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(vocab,filename):\n",
    "    \"Write all vocabs line by line in a file\"\n",
    "    with open(filename,'w',encoding='utf-8') as f:\n",
    "        for i, word in enumerate(vocab):\n",
    "            if i != len(vocab)-1:\n",
    "                f.write(\"{}\\n\".format(word))\n",
    "            else:\n",
    "                f.write(word)\n",
    "    print(\"written {s} tokens in {d}\".format(s=len(vocab), d=filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(filename):\n",
    "    \"\"\"Assign id to each word in vocab\n",
    "        returns dictionary\"\"\"\n",
    "    d = {}\n",
    "    with open(filename,encoding='utf-8') as f:\n",
    "        for idx, word in enumerate(f):\n",
    "            word = word.strip()\n",
    "            d[word] = idx\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_glove_vectors(vocab, glove_filename, filename, dim):\n",
    "    \"\"\"Bulding compressed file of vectors of words\n",
    "    that are present in dataset\"\"\"\n",
    "    embeddings = np.zeros([len(vocab),dim],dtype='float32')\n",
    "    with open(glove_filename,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            if word in vocab:\n",
    "                embedding = [float(x) for x in line[1:]]\n",
    "                word_idx = vocab[word]\n",
    "                embeddings[word_idx] = np.asarray(embedding)\n",
    "    np.savez_compressed(filename, embeddings=embeddings)\n",
    "\n",
    "def get_glove_vectors(filename):\n",
    "    \"\"\"Loads the saved numpy file (Embeddings)\"\"\"\n",
    "    with np.load(filename) as data:\n",
    "        return data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read eng sentences\n",
    "# with open('english_sentences_tatoeba.txt','r',encoding='utf-8') as f:\n",
    "#     lns = f.readlines()\n",
    "# print(len(lns),lns[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(lines, max_sents_per_example=6, n_examples=1000):\n",
    "    \"\"\"\n",
    "        Generates training data for deepsegment from list of sentences.\n",
    "        Parameters:\n",
    "        lines (list): Base sentences for data generation.\n",
    "        max_sents_per_example (int): Maximum number of sentences to be combined to form a single paragraph.\n",
    "        \n",
    "        n_examples (int): Number of training examples to be generated.\n",
    "        \n",
    "        Returns:\n",
    "        list, list: Training data and corresponding labels in BIOU format.\n",
    "    \"\"\"\n",
    "    x, y = [], []\n",
    "    \n",
    "    for current_i in tqdm(range(n_examples)):\n",
    "        x.append([])\n",
    "        y.append([])\n",
    "\n",
    "        chosen_lines = []\n",
    "        for _ in range(random.randint(1, max_sents_per_example)):\n",
    "            chosen_lines.append(random.choice(lines))\n",
    "        \n",
    "        chosen_lines = [bad_sentence_generator(line, remove_punctuation=random.randint(0, 3)) for line in chosen_lines]\n",
    "        \n",
    "        for line in chosen_lines:\n",
    "            words = line.strip().split()\n",
    "            for word_i, word in enumerate(words):\n",
    "                x[-1].append(word)\n",
    "                label = 'O'\n",
    "                if word_i == 0:\n",
    "                    label = 'B-sent'\n",
    "                y[-1].append(label)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "def bad_sentence_generator(sent, remove_punctuation = None):\n",
    "    \"\"\"\n",
    "        Returns sentence with completely/ partially removed punctuation.\n",
    "        Parameters:\n",
    "        sent (str): Sentence on which the punctuation removal operation is performed.\n",
    "        \n",
    "        remove_punctuation (int): removing punctuation completely if remove_punctuation ==0 or ==1, removing punctuation till a randomly selected point if remove_punctuation ==2\n",
    "        Returns:\n",
    "        str: Sentence with modified punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    if not remove_punctuation:\n",
    "        remove_punctuation = random.randint(0, 3)\n",
    "\n",
    "    break_point = random.randint(1, len(sent)-2)\n",
    "    lower_case = random.randint(0, 2)\n",
    "\n",
    "    if remove_punctuation <= 1:\n",
    "        # removing punctuation completely if remove_punctuation ==0 or ==1\n",
    "        sent = re.sub('['+string.punctuation+']', '', sent)\n",
    "    \n",
    "    elif remove_punctuation == 2:\n",
    "        # removing punctuation till a randomly selected point if remove_punctuation ==2\n",
    "        if random.randint(0,1) == 0:\n",
    "            sent = re.sub('['+string.punctuation+']', '', sent[:break_point]) + sent[break_point:]\n",
    "        # removing punctuation after a randomly selected point if remove_punctuation ==2        \n",
    "        else:\n",
    "            sent = sent[:break_point] + re.sub('['+string.punctuation+']', '', sent[break_point:])    \n",
    "    \n",
    "    if lower_case <= 1:\n",
    "        # lower casing sentence \n",
    "        sent = sent.lower()\n",
    "    \n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(lines, max_sents_per_example=6, n_examples=1000,punct=None):\n",
    "    \"\"\"\n",
    "        Generates training data for deepsegment from list of sentences.\n",
    "        Parameters:\n",
    "        lines (list): Base sentences for data generation.\n",
    "        max_sents_per_example (int): Maximum number of sentences to be combined to form a single paragraph.\n",
    "        \n",
    "        n_examples (int): Number of training examples to be generated.\n",
    "        \n",
    "        Returns:\n",
    "        list, list: Training data and corresponding labels in BIOU format.\n",
    "    \"\"\"\n",
    "    x, y = [], []\n",
    "    \n",
    "    for current_i in tqdm(range(n_examples)):\n",
    "        x.append([])\n",
    "        y.append([])\n",
    "\n",
    "        chosen_lines = []\n",
    "        for _ in range(random.randint(1, max_sents_per_example)):\n",
    "            chosen_lines.append(random.choice(lines))\n",
    "        \n",
    "        chosen_lines = [bad_sentence_generator(line, remove_punctuation=random.randint(0, 3),use_punct=punct) for line in chosen_lines]\n",
    "        \n",
    "        for line in chosen_lines:\n",
    "            words = line.strip().split()\n",
    "            for word_i, word in enumerate(words):\n",
    "                x[-1].append(word)\n",
    "                label = 'O'\n",
    "                if word_i == 0:\n",
    "                    label = 'B-sent'\n",
    "                y[-1].append(label)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "punct = ['And','Or','So','After','Once','Since','So','After that','Though',',',':',';','now','now then',\"\"]\n",
    "\n",
    "def bad_sentence_generator(sent, remove_punctuation = None,use_punct=None):\n",
    "    \"\"\"\n",
    "        Returns sentence with completely/ partially removed punctuation.\n",
    "        Parameters:\n",
    "        sent (str): Sentence on which the punctuation removal operation is performed.\n",
    "        \n",
    "        remove_punctuation (int): removing punctuation completely if remove_punctuation ==0 or ==1, removing punctuation till a randomly selected point if remove_punctuation ==2\n",
    "        Returns:\n",
    "        str: Sentence with modified punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    if not remove_punctuation:\n",
    "        remove_punctuation = random.randint(0, 3)\n",
    "\n",
    "    break_point = random.randint(1, len(sent)-2)\n",
    "    lower_case = random.randint(0, 2)\n",
    "\n",
    "    if remove_punctuation <= 1:\n",
    "        # removing punctuation completely if remove_punctuation ==0 or ==1\n",
    "        sent = re.sub('['+string.punctuation+']', '', sent)\n",
    "        if punct and remove_punctuation == 0:\n",
    "            sent = f\"{random.choice(punct)}\"+\" \"+sent\n",
    "    \n",
    "    elif remove_punctuation == 2:\n",
    "        # removing punctuation till a randomly selected point if remove_punctuation ==2\n",
    "        if random.randint(0,1) == 0:\n",
    "            sent = re.sub('['+string.punctuation+']', '', sent[:break_point]) + sent[break_point:]\n",
    "        # removing punctuation after a randomly selected point if remove_punctuation ==2        \n",
    "        else:\n",
    "            sent = sent[:break_point] + re.sub('['+string.punctuation+']', '', sent[break_point:])    \n",
    "    \n",
    "    if lower_case <= 1:\n",
    "        # lower casing sentence \n",
    "        sent = sent.lower()\n",
    "    \n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 34000/34000 [00:03<00:00, 10563.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 7000/7000 [00:00<00:00, 12043.92it/s]\n"
     ]
    }
   ],
   "source": [
    "x, y = generate_data(lines=lns[8000:], max_sents_per_example=6, n_examples=34000,punct=punct)\n",
    "x_, y_ = generate_data(lines=lns[:8000], max_sents_per_example=6, n_examples=7000,punct=punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glove.6B.100d.txt'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('glove')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(i,j) for i, j in zip(x,y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab ....\n",
      "-done vocab words 153--- vocab_tags 2\n",
      "written 2 tokens in data_folder/tags.txt\n",
      "written 50 tokens in data_folder.char.txt\n"
     ]
    }
   ],
   "source": [
    "#building vocab from glove and dataset \n",
    "#combine it \n",
    "#write it\n",
    "vocab_words, vocab_tags = get_vocabs([dataset])\n",
    "vocab_chars = get_char_vocab(dataset)\n",
    "build_vocab(vocab_tags, TAG_FILENAME)\n",
    "build_vocab(vocab_chars, CHAR_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab..glove...\n",
      "done. 400000 tokens in glove\n"
     ]
    }
   ],
   "source": [
    "vocab_glove = get_glove_vocab(GLOVE_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = final_vocab(vocab_words, vocab_glove, NUM,UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written 96 tokens in data_folder/vocabs.txt\n"
     ]
    }
   ],
   "source": [
    "build_vocab(vocab, VOCAB_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving ids to all vocab\n",
    "word_2_id = load_vocab(VOCAB_FILENAME)\n",
    "tag_2_id  = load_vocab(TAG_FILENAME)\n",
    "char_2_id = load_vocab(CHAR_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NWORDS     = len(vocab_words)\n",
    "NCHARS     = len(vocab_chars)\n",
    "NTAGS      = len(vocab_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving npz with embedding of vocab\n",
    "export_glove_vectors(vocab=word_2_id, glove_filename=GLOVE_FILEPATH, filename=EMBEDDING_VOCAB, dim=100)\n",
    "export_glove_vectors(vocab=tag_2_id, glove_filename=GLOVE_FILEPATH, filename=EMBEDDING_TAG, dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the glove vectors from saved npz\n",
    "word_embeddings = get_glove_vectors(filename=EMBEDDING_VOCAB)\n",
    "tag_embeddings = get_glove_vectors(filename=EMBEDDING_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting them into list of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processing_word(vocab_words=None, vocab_chars=None,\n",
    "                    lowercase=False, chars=False, allow_unk=True):\n",
    "    \"\"\"Return lambda function that transform a word (string) into list,\n",
    "    or tuple of (list, id) of int corresponding to the ids of the word and\n",
    "    its corresponding characters.\n",
    "\n",
    "    Args:\n",
    "        vocab: dict[word] = idx\n",
    "\n",
    "    Returns:\n",
    "        f(\"cat\") = ([12, 4, 32], 12345)\n",
    "                 = (list of char ids, word id)\n",
    "\n",
    "    \"\"\"\n",
    "    def f(word):\n",
    "        # 0. get chars of words\n",
    "        UNK = \"$UNK$\"\n",
    "        if vocab_chars is not None and chars == True:\n",
    "            char_ids = []\n",
    "            for char in word:\n",
    "                # ignore chars out of vocabulary\n",
    "                if char in vocab_chars:\n",
    "                    char_ids += [vocab_chars[char]]\n",
    "\n",
    "        # 1. preprocess word\n",
    "        if lowercase:\n",
    "            word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = NUM\n",
    "\n",
    "        # 2. get id of word\n",
    "        if vocab_words is not None:\n",
    "            if word in vocab_words:\n",
    "                word = vocab_words[word]\n",
    "            else:\n",
    "                if allow_unk:\n",
    "                    word = vocab_words[UNK]\n",
    "                else:\n",
    "                    raise Exception(\"Unknow key is not allowed. Check that \"\\\n",
    "                                    \"your vocab (tags?) is correct\")\n",
    "\n",
    "        # 3. return tuple char ids, word id\n",
    "        if vocab_chars is not None and chars == True:\n",
    "            return char_ids, word\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_words(\"gvjvgvj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_words = get_processing_word(vocab_words=word_2_id,vocab_chars=char_2_id,chars=True)\n",
    "process_tags = get_processing_word(vocab_words=tag_2_id,vocab_chars=char_2_id,chars=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLLDataset(object):\n",
    "    \"\"\"Class that iterates over CoNLL Dataset\n",
    "\n",
    "    __iter__ method yields a tuple (words, tags)\n",
    "        words: list of raw words\n",
    "        tags: list of raw tags\n",
    "\n",
    "    If processing_word and processing_tag are not None,\n",
    "    optional preprocessing is appplied\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        data = CoNLLDataset(filename)\n",
    "        for sentence, tags in data:\n",
    "            pass\n",
    "        ```\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data, processing_word=None, processing_tag=None,\n",
    "                 max_iter=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filename: path to the file\n",
    "            processing_words: (optional) function that takes a word as input\n",
    "            processing_tags: (optional) function that takes a tag as input\n",
    "            max_iter: (optional) max number of sentences to yield\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.processing_word = processing_word\n",
    "        self.processing_tag = processing_tag\n",
    "        self.max_iter = 32\n",
    "        self.length = None\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        niter = 0\n",
    "#         words, tags = [], []\n",
    "        for d in self.data:\n",
    "            words, tags = [], []\n",
    "            \n",
    "#             if self.max_iter == niter:\n",
    "#                 yield words, tags\n",
    "#                 words, tags = [], []\n",
    "#             else:\n",
    "#                 niter+=1\n",
    "            word, tag = d[0],d[-1]\n",
    "#             print(word),print(tag)\n",
    "            for word_, tag_ in zip(word,tag):\n",
    "                if self.processing_word is not None:\n",
    "                    word_ = self.processing_word(word_)\n",
    "                if self.processing_tag is not None:\n",
    "                    tag_ = self.processing_tag(tag_)\n",
    "                words += [word_]\n",
    "                tags += [tag_]\n",
    "            yield words, tags\n",
    "                \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Iterates once over the corpus to set and store length\"\"\"\n",
    "        if self.length is None:\n",
    "            self.length = 0\n",
    "            for _ in self:\n",
    "                self.length += 1\n",
    "\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('train.txt','w',encoding='utf-8') as f:\n",
    "#     for i,item in enumerate(zip(x,y)):\n",
    "#         if i != len(x)-1:\n",
    "#             f.write('{} {}\\n'.format(item[0],item[1]))\n",
    "#         else:\n",
    "#             f.write('{} {}'.format(item[0],item[1]))\n",
    "# with open('dev.txt','w',encoding='utf-8') as f:\n",
    "#     for i,item in enumerate(zip(x_,y_)):\n",
    "#         if i != len(x)-1:\n",
    "#             f.write('{} {}\\n'.format(item[0],item[1]))\n",
    "#         else:\n",
    "#             f.write('{} {}'.format(item[0],item[1]))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = [(i,j) for i, j in zip(x,y)]\n",
    "test_ = [(i,j) for i, j in zip(x_,y_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data   = CoNLLDataset(data=train_, processing_word = process_words,\n",
    "                     processing_tag = process_tags, max_iter=None)\n",
    "train_data = CoNLLDataset(data=test_, processing_word = process_words,\n",
    "                     processing_tag = process_tags, max_iter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 1.15.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\abhis\\anaconda3\\envs\\tensor2_0\\lib\\site-packages\n",
      "Requires: tensorflow-estimator, gast, wrapt, grpcio, numpy, wheel, protobuf, keras-preprocessing, opt-einsum, absl-py, keras-applications, termcolor, google-pasta, astor, tensorboard, six\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-c193d79f7d5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-f298bb8bdef5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#intitate placeholders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mword_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"word_id_placeholder\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sequence_length_placholder\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dropout\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "#intitate placeholders\n",
    "word_ids = tf.placeholder(tf.int32, shape=[None,None], name=\"word_id_placeholder\")\n",
    "sequence_length = tf.placeholder(tf.int32, shape=[None,], name=\"sequence_length_placholder\")\n",
    "label = tf.placeholder(tf.int32,shape=[None,None], name=\"labels\")\n",
    "dropout = tf.placeholder(dtype=tf.float32, shape=[], name=\"dropout\")\n",
    "lr = tf.placeholder(dtype=tf.float32, shape=[], name=\"lr\")\n",
    "char_id = tf.placeholder(dtype=tf.int32, shape=[None,None,None], name=\"char_id\")\n",
    "word_lengths = tf.placeholder(dtype=tf.int32, shape=[None,None], name=\"word_length_placeholder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "NWORDS = len(word_2_id.keys())\n",
    "print(NWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_HIDDEN_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding_fn(nwords,nchars=None,char_hidden_size=None,char_id=None,word_lengths=None,dropout=False,embedding_dict=None,embedding_size=100,word_ids=word_ids,use_char_embed=False,):\n",
    "    with tf.variable_scope(\"word_embedding\"):\n",
    "        if embedding_dict is None:\n",
    "            word_embeddings_ = tf.get_variable(name=\"word_embeddings_\",\n",
    "                                              dtype=tf.float32,\n",
    "                                              shape=[nwords,embedding_size])\n",
    "        else:\n",
    "            word_embeddings_ = tf.Variable(embedding_dict,\n",
    "                                           name=\"word_embeddings_\",\n",
    "                                           dtype=tf.float32,\n",
    "                                           shape=[nwords,embedding_size],\n",
    "                                           trainable=True\n",
    "                                           )\n",
    "            \n",
    "        word_embeddings = tf.nn.embedding_lookup(word_embeddings_,word_ids,name=\"word_embeddings\")\n",
    "    \n",
    "    with tf.variable_scope(\"char\"):\n",
    "        print('using char embedding')\n",
    "        if use_char_embed:\n",
    "            char_embeddings_ = tf.Variable(tf.random_uniform([nchars,50],-1.0,1.0),\n",
    "                        name=\"_char_embeddings\",\n",
    "                        dtype=tf.float32)\n",
    "                        #shape=[nchars, char_hidden_size])\n",
    "            char_embeddings = tf.nn.embedding_lookup(char_embeddings_,\n",
    "                                                    char_id,\n",
    "                                                     name=\"char_embeddings\")\n",
    "            \n",
    "            #including time dimesion\n",
    "            s = tf.shape(char_embeddings)\n",
    "            char_embeddings = tf.reshape(char_embeddings,\n",
    "                                        shape=[s[0]*s[1],s[-2],50])\n",
    "            word_length = tf.reshape(word_lengths,shape=[s[0]*s[1]])\n",
    "            \n",
    "            #lstm bidir over chars\n",
    "            fw_cell = tf.contrib.rnn.LSTMCell(char_hidden_size, state_is_tuple=True)\n",
    "            bw_cell = tf.contrib.rnn.LSTMCell(char_hidden_size, state_is_tuple=True)\n",
    "            \n",
    "            output_ = tf.nn.bidirectional_dynamic_rnn(fw_cell,bw_cell, char_embeddings,\n",
    "                                                     sequence_length = word_length, dtype=tf.float32)\n",
    "            \n",
    "            #read tthe output_\n",
    "            _, ((_,fw_output),(_,bw_output)) = output_\n",
    "            output = tf.concat([fw_output,bw_output],axis=-1)\n",
    "            #shape [bs, max_sequnce_length, char_hidden_size]\n",
    "            \n",
    "            output = tf.reshape(output,shape=[s[0],s[1],2*char_hidden_size])\n",
    "            word_embeddings = tf.concat([word_embeddings, output],axis=-1)\n",
    "    if dropout:\n",
    "        word_embeddings = tf.nn.dropout(word_embeddings, dropout)\n",
    "\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using char embedding\n"
     ]
    }
   ],
   "source": [
    "# word_embedding = word_embedding_fn(nwords=NWORDS,nchars=NCHARS, embedding_dict=word_embeddings,embedding_size=100)\n",
    "word_embedding = word_embedding_fn(nwords=NWORDS,nchars=NCHARS,char_hidden_size=CHAR_HIDDEN_DIM,char_id=char_id,word_lengths=word_lengths, embedding_dict=word_embeddings,embedding_size=100,use_char_embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_op(hidden_dim,word_embeddings,sequence_length,ntags=len(vocab_tags)):\n",
    "    with tf.variable_scope(\"bidirectional_lstm\"):\n",
    "        for_cell = tf.contrib.rnn.LSTMCell(hidden_dim)\n",
    "        bac_cell = tf.contrib.rnn.LSTMCell(hidden_dim)\n",
    "        (for_out, bac_out), _ = tf.nn.bidirectional_dynamic_rnn(for_cell, bac_cell,\n",
    "                                                                word_embeddings,\n",
    "                                                                sequence_length=sequence_length,\n",
    "                                                                dtype=tf.float32)\n",
    "        #shape >> [bs, sequence_length, 2EMD]\n",
    "        output = tf.concat([for_out, bac_out], axis=-1)\n",
    "\n",
    "    #         output = tf.nn.dropout(output,)\n",
    "\n",
    "    with tf.variable_scope(\"w_b\"):\n",
    "        W = tf.get_variable(name=\"W\",dtype=tf.float32,\n",
    "                            shape = [2*hidden_dim,ntags])\n",
    "        b = tf.get_variable(name=\"b\", dtype=tf.float32,\n",
    "                            shape = [ntags],\n",
    "                            initializer = tf.zeros_initializer())\n",
    "\n",
    "\n",
    "        nsteps = tf.shape(output)[1]\n",
    "        output = tf.reshape(output, [-1,2*hidden_dim])\n",
    "\n",
    "        pred = tf.matmul(output,W) + b\n",
    "        logits = tf.reshape(pred,[-1,nsteps,ntags])\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits_op(hidden_dim=HIDDEN_DIM,word_embeddings=word_embedding,sequence_length=sequence_length,ntags=len(vocab_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_op(logits):\n",
    "    label_pred = tf.cast(tf.argmax(logits, axis=-1),tf.int32)\n",
    "    return label_pred\n",
    "label_pred = pred_op(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_op(logits,labels,sequence_length,lr):\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=labels)\n",
    "    mask = tf.sequence_mask(sequence_length)\n",
    "    losses = tf.boolean_mask(losses, mask)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    return training_op, loss\n",
    "\n",
    "training_op, loss = loss_op(logits=logits, labels=label, sequence_length=sequence_length,lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_session():\n",
    "    sess = tf.Session()\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = initialize_session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_session():\n",
    "    if not os.path.exists('model_folder'):\n",
    "        os.makedirs('model_folder')\n",
    "    saver.save(sess, 'model_folder')\n",
    "\n",
    "def close_sess():\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minibatches of data\n",
    "def minibatches(data,batch_size):\n",
    "    x_batch, y_batch = [], []\n",
    "    for (x,y) in data:\n",
    "        if len(x_batch) == batch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "        if type(x[0]) == tuple:\n",
    "            x = zip(*x)\n",
    "        x_batch += [x]\n",
    "        y_batch += [y]\n",
    "        \n",
    "    if len(x_batch) != 0:\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "    \"\"\"\n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type\n",
    "\n",
    "\n",
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "    \"\"\"\n",
    "    if NONE in tags:\n",
    "        default = tags[NONE]\n",
    "    else:\n",
    "        default = None\n",
    "        \n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "#         print(tok)\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(words):\n",
    "    fd, sequence_length = get_feed_dict(words,use_chars=True) #dropout=1.0)\n",
    "    labels_pred = sess.run(label_pred, feed_dict=fd)\n",
    "    return labels_pred, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_tok, nlevels=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "        nlevels: \"depth\" of padding, for the case where we have characters ids\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    if nlevels == 1:\n",
    "        max_length = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, sequence_length = _pad_sequences(sequences,\n",
    "                                            pad_tok, max_length)\n",
    "\n",
    "    elif nlevels == 2:\n",
    "        max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "                               for seq in sequences])\n",
    "        sequence_padded, sequence_length = [], []\n",
    "        for seq in sequences:\n",
    "            # all words are same length now\n",
    "            sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\n",
    "            sequence_padded += [sp]\n",
    "            sequence_length += [sl]\n",
    "\n",
    "        max_length_sentence = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, _ = _pad_sequences(sequence_padded,\n",
    "                [pad_tok]*max_length_word, max_length_sentence)\n",
    "        sequence_length, _ = _pad_sequences(sequence_length, 0,\n",
    "                max_length_sentence)\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(words, labels=None, lr=None, dropout=None,use_chars=False):\n",
    "        \"\"\"Given some data, pad it and build a feed dictionary\n",
    "        Args:\n",
    "            words: list of sentences. A sentence is a list of ids of a list of\n",
    "                words. A word is a list of ids\n",
    "            labels: list of ids\n",
    "            lr: (float) learning rate\n",
    "            dropout: (float) keep prob\n",
    "        Returns:\n",
    "            dict {placeholder: value}\n",
    "        \"\"\"\n",
    "        #perform padding of the given data\n",
    "        if use_chars:\n",
    "            char_ids, word_ids = zip(*words)\n",
    "            word_ids, sequence_lengths = pad_sequences(word_ids, 0)\n",
    "            char_ids, word_lengths = pad_sequences(char_ids, pad_tok=0,\n",
    "                nlevels=2)\n",
    "        else:\n",
    "            word_ids, sequence_lengths = pad_sequences(words, 0)\n",
    "\n",
    "        # build feed dictionary\n",
    "        feed = {\n",
    "            \"word_id_placeholder:0\": word_ids,\n",
    "            \"sequence_length_placholder:0\": sequence_lengths\n",
    "        }\n",
    "\n",
    "        if use_chars:\n",
    "            feed[\"char_id:0\"] = char_ids\n",
    "            feed[\"word_length_placeholder:0\"] = word_lengths\n",
    "        if labels is not None:\n",
    "            labels, _ = pad_sequences(labels, 0)\n",
    "            feed[\"labels:0\"] = labels\n",
    "\n",
    "        if lr is not None:\n",
    "            feed[\"lr:0\"] = lr\n",
    "\n",
    "        if dropout is not None:\n",
    "            feed[\"dropout:0\"] = dropout\n",
    "\n",
    "        return feed, sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(train, dev, epoch,batch_size=128):\n",
    "    for i ,(words, labels) in tqdm(enumerate(minibatches(train,batch_size))):\n",
    "        fd, _= get_feed_dict(words, labels, LR, DROPOUT,use_chars=True)\n",
    "#         print(\"......................................\")\n",
    "        \n",
    "        _, train_los = sess.run([training_op, loss], feed_dict = fd)\n",
    "        print(\"training_loss::::\",train_los)\n",
    "        \n",
    "    metrics = evaluation(dev)\n",
    "    msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "            for k, v in metrics.items()])\n",
    "    print(msg)\n",
    "    print(metrics['f1'])\n",
    "    return metrics['f1']\n",
    "        \n",
    "\n",
    "def evaluation(dev):\n",
    "    accs = []\n",
    "    correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "    for words, labels in minibatches(dev, batch_size=32):\n",
    "        label_preds, sequence_length = predict_batch(words)\n",
    "        \n",
    "        for lab, lab_pred, length in zip(labels, label_preds, sequence_length):\n",
    "            lab = lab[:length]\n",
    "            lab_pred = lab_pred[:length]\n",
    "            \n",
    "            accs += [a==b for (a,b) in zip(lab,lab_pred)]\n",
    "#             print(\"lab::\",lab,\"fininsh\")\n",
    "            lab_chunks = set(get_chunks(lab, tag_2_id))\n",
    "#             print(\"lab_pred\",lab_pred)\n",
    "            lab_pred_chunks = set(get_chunks(lab_pred, tag_2_id))\n",
    "            \n",
    "            correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "            total_preds   += len(lab_pred_chunks)\n",
    "            total_correct += len(lab_chunks)\n",
    "            \n",
    "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "    acc = np.mean(accs)\n",
    "\n",
    "    return {\"acc\": 100*acc, \"f1\": 100*f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train, dev, epochs, epoch_no_improvement=EPOCHS_NO_IMPROVEMENT):\n",
    "    global LR\n",
    "    best_score = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch {epoch} out of {epochs}\")\n",
    "        score = run_epoch(train, dev, epoch)\n",
    "        LR *= 0.9#lr_decay # decay learning rate\n",
    "        if score >= best_score:\n",
    "            epo_no_improvement = 0\n",
    "            save_session()\n",
    "            best_score = score\n",
    "            print(\"Best Score\", best_score)\n",
    "        else:\n",
    "            epo_no_improvement += 1\n",
    "            if epo_no_improvement > epoch_no_improvement:\n",
    "                print(f\"Early Stoping {epo_no_improvement} with no Improvement\")\n",
    "                break\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = [(i,j) for i, j in zip(x,y)]\n",
    "# dev_data = [(i,j) for i, j in zip(x_,y_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.7699002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:08,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.394743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:16,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.42662278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:25,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.42495412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:35,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.3922895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:42,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.3415032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:51,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.31880867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [01:00,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.3211897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [01:07,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.3284933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [01:15,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.2953337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [01:24,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.26696548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [01:34,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.2628289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [01:41,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.24101132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [01:50,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.22767223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [01:58,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.22477312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [02:05,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.1966457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [02:13,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.18856484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [02:21,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.16653448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [02:30,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.14644077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [02:39,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.13401395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [02:47,  8.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.13097543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [02:55,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.11357886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [03:03,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.10166957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [03:11,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.08997118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [03:18,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.07657217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "25it [03:27,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0729999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "26it [03:36,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.06843629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "27it [03:44,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.065625794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "28it [03:52,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.07079851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "29it [04:00,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.045037024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "30it [04:08,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.05936083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "31it [04:18,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.051698744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "32it [04:25,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.05726403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "33it [04:33,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.065165706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "34it [04:41,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.050791465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "35it [04:50,  8.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.043726824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "36it [05:01,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.042911507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "37it [05:10,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.04643424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "38it [05:19,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.03658593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "39it [05:27,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.031063832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "40it [05:35,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.040431548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "41it [05:43,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.03884462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "42it [05:51,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.025326194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "43it [05:59,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.02970109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "44it [06:07,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.020138854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "45it [06:15,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.027404524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "46it [06:23,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.021304144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "47it [06:31,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.020645374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "48it [06:39,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.023499599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "49it [06:48,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.017877243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "50it [06:56,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.014773175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "51it [07:03,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.014899133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "52it [07:11,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.014327517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "53it [07:20,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.015901856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "54it [07:28,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.015156472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [07:34,  8.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 97.00 - f1 84.42\n",
      "84.42410597605542\n",
      "Best Score 84.42410597605542\n",
      "epoch 1 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.01609655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:07,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.010040144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:15,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.014927541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:24,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.009113354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:34,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.010658251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:42,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.011765984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:51,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.009668988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [01:00,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.014498273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [01:07,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.00768126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [01:15,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0054243403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [01:24,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.009596468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [01:34,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.011641596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [01:42,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.013211692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [01:50,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.012169876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [01:58,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0085504735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [02:06,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.01030514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [02:14,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0111147575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [02:22,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.007297908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [02:30,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.008322038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [02:39,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.009820469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [02:48,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0068513653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [02:56,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.009070322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [03:04,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.006254731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [03:12,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.004964836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [03:20,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.007617996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "25it [03:28,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.005760604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "26it [03:36,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.003920912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "27it [03:44,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.006649924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "28it [03:51,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0067363842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "29it [03:59,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0038302764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "30it [04:06,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.008460445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "31it [04:15,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.006883854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "32it [04:22,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.005080871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "33it [04:30,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.008342123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "34it [04:38,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.007878918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "35it [04:45,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.003497989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "36it [04:54,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.005790295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "37it [05:02,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.006032504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "38it [05:10,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.003888753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "39it [05:18,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0048209485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "40it [05:25,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0055636507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "41it [05:33,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.004892945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "42it [05:41,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0042543705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "43it [05:48,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.003600629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "44it [05:57,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0033453254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "45it [06:04,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0047990084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "46it [06:12,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0028151658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "47it [06:19,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0035910215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "48it [06:27,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0045265686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "49it [06:36,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0030891832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "50it [06:44,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0039800713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "51it [06:51,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.002715759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "52it [06:59,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0024431024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "53it [07:08,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0032585573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "54it [07:16,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:::: 0.0033750576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [07:23,  8.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 97.57 - f1 87.73\n",
      "87.7321330591379\n",
      "Best Score 87.7321330591379\n",
      "epoch 2 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:08, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-affb30350b9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_no_improvement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-06ef973a5c1c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train, dev, epochs, epoch_no_improvement)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"epoch {epoch} out of {epochs}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mLR\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;31m#lr_decay # decay learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-e46ee17fd906>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(train, dev, epoch, batch_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#         print(\"......................................\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_los\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training_loss::::\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_los\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensor1_15\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train(train=train_data, dev=dev_data, epochs = 25, epoch_no_improvement=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "def predict(words_raw):\n",
    "    \"\"\"Returns list of tags\n",
    "\n",
    "    Args:\n",
    "        words_raw: list of words (string), just one sentence (no batch)\n",
    "\n",
    "    Returns:\n",
    "        preds: list of tags (string), one for each word in the sentence\n",
    "\n",
    "    \"\"\"\n",
    "    words = [process_words(w) for w in words_raw.split(\" \")]\n",
    "    if type(words[0]) == tuple:\n",
    "        words = zip(*words)\n",
    "    pred_ids, _ = predict_batch([words])\n",
    "    preds = [idx_to_tag[idx] for idx in list(pred_ids[0])]\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_tag = {v:k for k,v in tag_2_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hey if MLSDREG is set then read the bit MLSDBITVAL of MLSDREG and clear the MLSDREG then set the MLSDREG to MLSDVAL else if MLSDREG is set to MLSDVAL then Write MLSDVAL to bit MLSDBITVAL of MLSDREG\"#\"i want to check if MLSDREG is initialized to MLSDVAL and then clear the register MLSDREG's MLSDFIELD and then set MLSDREG\"\n",
    "preds = predict(words_raw=text)\n",
    "start = 0\n",
    "sente=[]\n",
    "for p,w in zip(preds,text.split(\" \")):\n",
    "#     print(w)\n",
    "#     print(type(w))\n",
    "    sent = \"\"\n",
    "    if start == 1 and p == 'B-sent':\n",
    "        sente.append(sent)\n",
    "        sent = \"\"\n",
    "        start = 0\n",
    "        continue\n",
    "    if p == 'B-sent' and start == 0:\n",
    "        start = 1\n",
    "        sent += str(w) + \" \"\n",
    "        continue\n",
    "    if start == 1 and p == \"O\":\n",
    "        sent += str(w) + \" \"\n",
    "        continue\n",
    "    if start ==0 and p == \"O\":\n",
    "        sent += str(w) + \" \"\n",
    "        continue\n",
    "sente.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-sent ____ hey\n",
      "O ____ if\n",
      "O ____ MLSDREG\n",
      "O ____ is\n",
      "O ____ set\n",
      "O ____ then\n",
      "O ____ read\n",
      "O ____ the\n",
      "O ____ bit\n",
      "O ____ MLSDBITVAL\n",
      "O ____ of\n",
      "O ____ MLSDREG\n",
      "B-sent ____ and\n",
      "O ____ clear\n",
      "O ____ the\n",
      "O ____ MLSDREG\n",
      "O ____ then\n",
      "O ____ set\n",
      "O ____ the\n",
      "O ____ MLSDREG\n",
      "O ____ to\n",
      "O ____ MLSDVAL\n",
      "B-sent ____ else\n",
      "O ____ if\n",
      "O ____ MLSDREG\n",
      "O ____ is\n",
      "O ____ set\n",
      "O ____ to\n",
      "O ____ MLSDVAL\n",
      "O ____ then\n",
      "O ____ Write\n",
      "O ____ MLSDVAL\n",
      "O ____ to\n",
      "O ____ bit\n",
      "O ____ MLSDBITVAL\n",
      "O ____ of\n",
      "O ____ MLSDREG\n"
     ]
    }
   ],
   "source": [
    "for p,w in zip(preds,text.split(\" \")):\n",
    "    print(p,\"____\",w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
