{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build params (Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "DECAY_LR = 0.9\n",
    "OPTM = \"Adam\"\n",
    "DROPOUT = 0.3\n",
    "EPOCHS_NO_IMPROVEMENT = 4\n",
    "HIDDEN_DIM = 256\n",
    "DATA_FILEPATH = \"data_folder/data.txt\"\n",
    "GLOVE_FILEPATH = \"glove/\"\n",
    "VOCAB_FILENAME = \"data_folder/vocabs.txt\"\n",
    "TAG_FILENAME = \"data_folder/tags.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build files from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_batches(data,batch_size=32):\n",
    "    \"\"\"Args >> data=tuple(sentence,tag)\n",
    "    returns data in batches\"\"\"\n",
    "    steps = len(data)//batch_size\n",
    "    x_batch, y_batch = [], []\n",
    "    for (x,y) in data:\n",
    "        if len(x_batch) == batch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "        if type(x[0])==tuple:\n",
    "            x = zip(*x)\n",
    "            x_batch += [x]\n",
    "            y_batch += [y]\n",
    "    if len(x_batch) != 0:\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "NONE = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabs(datasets):\n",
    "    \"\"\"Building vocabs present in datasets\"\"\"\n",
    "    print(\"Building Vocab ....\")\n",
    "    vocab_words = set()\n",
    "    vocab_tags = set()\n",
    "    for dataset in datasets:\n",
    "        for words, tags in dataset:\n",
    "            vocab_words.update(words)\n",
    "            vocab_tags.update(tags)\n",
    "    print(\"-done vocab words {s}--- vocab_tags {d}\".format(s=vocab_words,d=vocab_tags))\n",
    "    return vocab_words, vocab_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_vocab(path):\n",
    "    \"\"\"Building vocabs present in glove\"\"\"\n",
    "    print(\"Building Vocab..glove...\")\n",
    "    vocab = set()\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            word = line.strip().split(' ')[0]\n",
    "            vocab.add(word)\n",
    "    print(\"done. {} tokens in glove\".format(len(vocab)))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_vocab(vocab_words,vocab_glove,**args):\n",
    "    \"\"\"Combine word vocabs >> glove_vocabs+dataset_vocabs\"\"\"\n",
    "    vocab = vocab_words & vocab_glove\n",
    "    for word in args:\n",
    "        vocab.add(word)\n",
    "    return vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab():\n",
    "    \"Write all vocabs line by line in a file\"\n",
    "    with open(filename,'w'):\n",
    "        for i, word in enumerate(voacb):\n",
    "            if i != len(vocab)-1:\n",
    "                f.write(\"{}\\n\".format(word))\n",
    "            else:\n",
    "                f.write(word)\n",
    "    print(\"written {s} tokens in {d}\".format(s=len(vocab), d=filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(filename):\n",
    "    \"\"\"Assign id to each word in vocab\n",
    "        returns dictionary\"\"\"\n",
    "    d = {}\n",
    "    with open(filename) as f:\n",
    "        for idx, word in enumerate(f):\n",
    "            word = word.strip()\n",
    "            d[word] = idx\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_glove_vectors(vocab, glove_filename, filename, dim):\n",
    "    \"\"\"Bulding compressed file of vectors of words\n",
    "    that are present in dataset\"\"\"\n",
    "    embeddings = np.zeroes(len(vocab),dim)\n",
    "    with open(glove_filename,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            if word in vocab:\n",
    "                embedding = [float(x) for x in line[1:]]\n",
    "                word_idx = vocab[word]\n",
    "                embeddings[word_idx] = np.asarray(embedding)\n",
    "    np.savez_compressed(filename, embeddings=embeddings)\n",
    "\n",
    "def get_glove_vectors(filename):\n",
    "    \"\"\"Loads the saved numpy file (Embeddings)\"\"\"\n",
    "    with np.load(filename) as data:\n",
    "        return data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
